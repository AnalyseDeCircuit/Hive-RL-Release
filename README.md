# Hive Game Python Project

[中文 | [English](./README.en.md) | [Français](./README.fr.md) | [Español](./README.es.md)]

## 项目简介
本项目是一个基于 Python 实现的 Hive（蜂巢棋）桌面策略游戏，支持人类玩家对战、AI 对战、AI 训练与评估等多种模式。项目兼容基础棋子与 DLC 棋子扩展，具备完整的棋盘逻辑、玩家管理、AI 智能决策与神经网络训练能力。

## 主要功能
- **人类 vs 人类**：支持两名玩家本地对战，完整还原 Hive 棋规则。
- **人类 vs AI**：支持人机对战，AI 可加载训练模型或随机走子。
- **AI 训练**：可自动进行自我博弈训练，强化学习提升 AI 水平。
- **AI 评估**：支持批量评测 AI 水平。
- **棋盘与规则**：实现了 Hive 棋所有基础与 DLC 棋子、落子与移动规则、胜负判定等。

## 技术架构
- **语言**：Python 3
- **主要模块**：
  - `main.py`：主入口，菜单与主循环
  - `game.py`：游戏主流程与状态管理
  - `player.py` / `ai_player.py`：玩家与 AI 玩家对象
  - `board.py`：棋盘与棋子管理
  - `hive_env.py` / `game_state.py`：AI 环境与状态编码
  - `neural_network.py`：AI 神经网络实现
  - `ai_trainer.py` / `ai_evaluator.py`：AI 训练与评估
  - `utils.py`：常量与辅助函数
- **AI 算法**：强化学习（Q-Learning/DQN），自定义神经网络，经验回放
- **数据结构**：面向对象设计，支持 clone、深拷贝、状态模拟

## 运行方式
1. 安装依赖：
   ```bash
   pip install -r requirements.txt
   ```
2. 启动游戏：
   ```bash
   python main.py
   ```
3. 按菜单选择对战/训练/评估等模式。

## 适用场景
- 桌面策略游戏开发与 AI 研究
- 强化学习与博弈 AI 实践
- Python 面向对象与项目架构学习

## 其他说明
- 支持基础棋子与 DLC 棋子（瓢虫、蚊子、pillbug）
- 代码结构清晰，便于二次开发与算法扩展
- 详细技术问题与修复记录见 `Q&S.md`

## 模块层次与结构说明

本项目采用分层、解耦的面向对象架构，主要分为以下层次：

- **界面与主流程层**
  - `main.py`：负责主菜单、用户交互、游戏主循环，调度各核心模块。

- **游戏规则与状态层**
  - `game.py`：封装游戏主流程、回合切换、胜负判定、玩家管理等。
  - `board.py`：棋盘数据结构与棋子落子/移动/判定逻辑。
  - `player.py`：人类玩家对象，管理棋子库存、落子/移动操作。
  - `piece.py`：棋子类型、属性与行为定义。

- **AI与环境层**
  - `ai_player.py`：AI 玩家对象，继承自 Player，集成强化学习决策与神经网络。
  - `hive_env.py`：AI 训练环境，封装状态、动作空间、奖励等接口，兼容 OpenAI Gym 风格。
  - `game_state.py`：游戏状态编码与特征提取，供 AI 输入。

- **AI训练与评估层**
  - `ai_trainer.py`：AI 自我博弈训练主控，负责采集经验、更新模型。
  - `ai_evaluator.py`：AI 水平评估与对局统计。
  - `neural_network.py`：自定义神经网络实现，用于状态价值/动作价值预测。

- **工具与常量层**
  - `utils.py`：棋子常量、辅助函数、全局配置等。

各层之间通过对象组合与接口调用解耦，便于维护和扩展。

---

## AI 强化学习设计原理（详细）

本项目的 AI 采用了基于 Q-Learning/DQN 的强化学习方法，结合自定义神经网络实现。具体细节如下：

### 1. 状态空间编码
- 每个游戏状态被编码为一个 814 维的特征向量：
  - 800 维：10x10 棋盘，每格用 one-hot 表示顶层棋子的类型（8种棋子）
  - 10 维：双方手牌剩余棋子数（5种基础棋子，归一化）
  - 4 维：当前玩家、回合数、双方女王是否已落子等全局信息
- 编码逻辑见 `game_state.py` 和 `ai_player.py` 的 `_get_observation_from_game_state` 方法。

### 2. 动作空间设计
- 所有合法落子和移动均被编码为离散整数（Action.encode_*），包括：
  - 落子：指定棋子类型和坐标
  - 移动：指定起点和终点
- AI 通过遍历所有可能动作并筛选合法性，生成当前可选动作集。

### 3. 神经网络结构
- 使用自定义多层感知机（MLP），结构为：
  - 输入层：814 维
  - 隐藏层：256 单元，ReLU 激活
  - 输出层：1 维，表示当前状态的价值（可扩展为动作价值 Q(s,a)）
- 网络实现见 `neural_network.py`。

### 4. 策略与探索
- 采用 epsilon-greedy 策略：
  - 以概率 epsilon 随机选择动作（探索），其余时间选择神经网络预测价值最高的动作（利用）。
  - epsilon 在训练过程中逐步衰减（如每局乘以 0.995），最终趋于 0.01，平衡探索与收敛。

### 5. 经验回放与批量训练
- 每步对局经验（s, a, r, s', done）存入回放池（replay buffer）。
- 每次训练时，随机采样一批经验，计算目标值：
  - 若未终局，目标 = r + γ * V(s')
  - 若已终局，目标 = r
- 用均方误差损失训练神经网络，提升样本利用率，减少相关性。

### 7. 自我博弈与模型更新
- AI 通过自我对弈不断积累经验，周期性保存模型参数（如每100局保存一次）。
- 训练完成后可加载模型用于对战或评估。

### 8. 评估与泛化
- 训练后通过 `ai_evaluator.py` 进行批量对局，统计胜率、平均步数等指标，评估 AI 水平。
- 支持不同 epsilon、模型参数下的对比实验。

---

如有建议或 bug 反馈，欢迎 issue 或 PR！
