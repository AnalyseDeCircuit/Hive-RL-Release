# Epsilon 衰减分析结果

## 问题确认

你说得对！从训练日志显示的 `Epsilon: 0.4147` 可以看出，当前的epsilon衰减确实还是按照旧的逻辑在运行。

## 问题根源

1. **Epsilon计算逻辑错误**：原代码中 `episodes_in_phase = episode` 使用的是全局episode计数，而不是当前阶段内的episode数
2. **衰减过快**：即使修改了配置，实际执行时可能还在使用旧的快速衰减逻辑

## 新的Epsilon衰减配置（已修改）

### Foundation阶段（前40,000轮）
- **起始epsilon**: 0.9 
- **结束epsilon**: 0.8
- **衰减周期**: 35,000轮
- **特点**: 35,000轮内只从0.9降到0.8（降幅仅0.1），保持高探索率

### Strategy阶段（40,001-90,000轮）
- **起始epsilon**: 0.8
- **结束epsilon**: 0.4  
- **衰减周期**: 40,000轮
- **特点**: 在基本规则掌握后开始策略学习

### Mastery阶段（90,001-120,000轮）
- **起始epsilon**: 0.4
- **结束epsilon**: 0.1
- **衰减周期**: 25,000轮

## 预期vs实际

### 在5,400轮时
- **新配置预期**: epsilon ≈ 0.885 (从0.9只降了一点点)
- **实际显示**: 0.4147 (说明还在用旧逻辑)

### 旧配置问题
- 在15,000轮内从0.9降到0.5，降幅0.4，速度过快
- 导致AI在还没学会基本规则时就减少了探索

## 解决方案

1. **立即重启训练**：应用新的epsilon衰减逻辑
2. **验证修复**：确保新的epsilon计算逻辑生效
3. **监控指标**：
   - `must_place_queen_violation` 比例应显著下降
   - `queen_surrounded` 事件应开始出现（表示AI学会了基本战术）

## 为什么没有queen_surrounded

在前期阶段（5,400轮，epsilon=0.4147），AI主要在学习：
1. **基本合法性**：不违反必须放置蜂后规则
2. **棋子放置**：学习有效的放置位置
3. **基础移动**：掌握移动规则

`queen_surrounded` 是更高级的战术结果，需要AI：
1. 首先掌握基本规则（减少违规）
2. 学会有目的地移动和放置
3. 理解包围战术

在新的慢速epsilon衰减下，AI将有更多时间在高探索率下学习这些基础技能。

---

**建议**：重新开始训练，或者从当前检查点继续但确保新的epsilon逻辑生效。预期在10,000-15,000轮后开始看到queen_surrounded事件。
