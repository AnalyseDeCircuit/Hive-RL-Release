[üá¨üáß English](README.en.md) | [üá®üá≥ ‰∏≠Êñá](README.md) | [üá™üá∏ Espa√±ol](README.es.md)

# Hive-RL : IA Hive bas√©e sur l'Apprentissage par Renforcement

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

## üìñ Introduction

Hive-RL est un projet avanc√© d'apprentissage par renforcement d√©di√© √† l'entra√Ænement d'une IA de haut niveau pour le jeu **Hive**. Ce projet utilise des techniques modernes d'apprentissage par renforcement profond, impl√©mentant un moteur de jeu complet, un syst√®me de r√©compenses scientifique et divers algorithmes d'entra√Ænement avanc√©s.

**Hive** est un jeu de strat√©gie prim√© qui ne n√©cessite pas de plateau, avec des r√®gles simples mais une profondeur strat√©gique exceptionnelle. L'objectif des joueurs est d'encercler la reine adverse en pla√ßant et d√©pla√ßant diverses pi√®ces d'insectes.

## ‚ú® Fonctionnalit√©s Principales

### üéÆ Moteur de Jeu Complet
- **Impl√©mentation pr√©cise des r√®gles** : Conforme aux r√®gles officielles de Hive
- **Support des extensions DLC** : Inclut la coccinelle, le moustique, le cloporte et autres pi√®ces officielles
- **Plateau haute performance** : Structures de donn√©es optimis√©es et acc√©l√©ration Numba
- **Validation des actions** : V√©rification stricte de la l√©galit√© et gestion d'erreurs

### üß† Syst√®me IA Avanc√©
- **Deep Q-Network (DQN)** : Architecture de r√©seau neuronal moderne bas√©e sur PyTorch
- **Fa√ßonnage de r√©compenses scientifique** : Syst√®me de r√©compenses multi-niveaux soigneusement con√ßu
- **Replay d'exp√©rience** : R√©utilisation efficace des √©chantillons et stabilit√© d'apprentissage
- **Strat√©gie Œµ-greedy** : Strat√©gie dynamique √©quilibrant exploration et exploitation

### üöÄ Framework d'Entra√Ænement Haute Performance
- **Auto-jeu parall√®le** : √âchantillonnage parall√®le multi-processus, am√©lioration significative de l'efficacit√© d'entra√Ænement
- **Apprentissage par curriculum** : Apprentissage progressif des r√®gles de base aux strat√©gies avanc√©es
- **Entra√Ænement adversarial** : Am√©lioration de la robustesse de l'IA par √©chantillons adversariaux
- **Fusion de mod√®les** : Syst√®me de d√©cision par vote multi-mod√®les

### üìä Visualisation et Analyse
- **Surveillance en temps r√©el** : Courbes de r√©compenses, pertes et taux de victoire pendant l'entra√Ænement
- **Analyse de performance** : Statistiques d√©taill√©es de fin de partie et analyse comportementale
- **√âvaluation de mod√®le** : Tests de performance automatis√©s

## üèóÔ∏è Architecture du Projet

```
Hive-RL/
‚îú‚îÄ‚îÄ Moteur Principal
‚îÇ   ‚îú‚îÄ‚îÄ game.py              # Logique principale du jeu
‚îÇ   ‚îú‚îÄ‚îÄ board.py             # Repr√©sentation et op√©rations du plateau
‚îÇ   ‚îú‚îÄ‚îÄ piece.py             # Types de pi√®ces et r√®gles de mouvement
‚îÇ   ‚îî‚îÄ‚îÄ player.py            # Classe de base des joueurs
‚îú‚îÄ‚îÄ Apprentissage par Renforcement
‚îÇ   ‚îú‚îÄ‚îÄ hive_env.py          # Environnement Gymnasium
‚îÇ   ‚îú‚îÄ‚îÄ ai_player.py         # Impl√©mentation du joueur IA
‚îÇ   ‚îú‚îÄ‚îÄ neural_network.py    # Architecture du r√©seau neuronal
‚îÇ   ‚îî‚îÄ‚îÄ improved_reward_shaping.py  # Syst√®me de fa√ßonnage des r√©compenses
‚îú‚îÄ‚îÄ Framework d'Entra√Ænement
‚îÇ   ‚îú‚îÄ‚îÄ ai_trainer.py        # Entra√Æneur principal
‚îÇ   ‚îú‚îÄ‚îÄ parallel_sampler.py  # √âchantillonneur parall√®le
‚îÇ   ‚îî‚îÄ‚îÄ ai_evaluator.py      # √âvaluateur de performance
‚îú‚îÄ‚îÄ Outils d'Analyse
‚îÇ   ‚îú‚îÄ‚îÄ analyze_model.py     # Analyse de mod√®le
‚îÇ   ‚îî‚îÄ‚îÄ plot_*.py           # Outils de visualisation
‚îî‚îÄ‚îÄ Interface Utilisateur
    ‚îî‚îÄ‚îÄ main.py             # Menu principal
```

## üöÄ D√©marrage Rapide

### Pr√©requis
- Python 3.10+
- PyTorch 2.0+
- NumPy, Matplotlib, Gymnasium
- Numba (optimisation des performances)

### Installation des D√©pendances
```bash
pip install -r requirements.txt
```

### Lancement du Projet
```bash
python main.py
```

### Options du Menu Principal
1. **Human vs Human** - Combat local √† deux joueurs
2. **Human vs AI** - Combat homme-machine
3. **AI Training** - Entra√Ænement de l'IA
4. **Evaluate AI & Plots** - √âvaluation des performances
5. **Exit Game** - Quitter

## üéØ Entra√Ænement de l'IA

### Modes d'Entra√Ænement
1. **Entra√Ænement de base par √©chantillonnage parall√®le** - Entra√Ænement multi-processus efficace
2. **Entra√Ænement de raffinement par auto-jeu** - Optimisation strat√©gique approfondie
3. **Entra√Ænement par vote d'ensemble** - Fusion multi-mod√®les
4. **Entra√Ænement de robustification adversariale** - Am√©lioration de la r√©sistance aux perturbations
5. **Apprentissage par curriculum** - Acquisition progressive des comp√©tences

### Phases d'Apprentissage par Curriculum
- **Foundation (0-40k √©pisodes)** - Apprentissage des r√®gles de base
- **Strategy (40k-90k √©pisodes)** - D√©veloppement de la pens√©e strat√©gique
- **Mastery (90k-120k √©pisodes)** - Ma√Ætrise des strat√©gies avanc√©es

### Caract√©ristiques d'Entra√Ænement
- **Sauvegarde automatique** : Progression d'entra√Ænement sauvegard√©e en temps r√©el, support de reprise
- **Surveillance des performances** : Affichage en temps r√©el de la vitesse d'entra√Ænement et de l'√©tat de convergence
- **Planification intelligente** : Ajustement dynamique d'epsilon et du taux d'apprentissage
- **Optimisation multi-processus** : 10 workers parall√®les, am√©lioration de 10x de la vitesse d'entra√Ænement

## üî¨ Principes Techniques

### Framework d'Apprentissage par Renforcement
- **Espace d'√©tats** : Vecteur de 820 dimensions incluant l'√©tat du plateau, informations de main, progression du jeu
- **Espace d'actions** : 20 000 actions discr√®tes couvrant tous les placements et mouvements possibles
- **Syst√®me de r√©compenses** : Conception de r√©compenses multi-niveaux, de la survie de base aux strat√©gies avanc√©es

### Syst√®me de Fa√ßonnage des R√©compenses
```python
R√©compenses Terminales (Poids : 60-63%)
‚îú‚îÄ‚îÄ Victoire: +5.0 + bonus de vitesse
‚îú‚îÄ‚îÄ D√©faite: -6.0 (reine encercl√©e)
‚îú‚îÄ‚îÄ Timeout: -3.0 (p√©nalit√© de d√©lai)
‚îî‚îÄ‚îÄ Match nul: Ajustement fin selon l'avantage

R√©compenses Strat√©giques (Poids : 25-40%)
‚îú‚îÄ‚îÄ Progr√®s d'encerclement: R√©compenses progressives
‚îú‚îÄ‚îÄ Am√©lioration d√©fensive: R√©compenses de position s√ªre
‚îî‚îÄ‚îÄ Coordination des pi√®ces: √âvaluation de la valeur positionnelle

R√©compenses de Base (Poids : 5-15%)
‚îú‚îÄ‚îÄ R√©compense de survie: Valeur positive minimale
‚îî‚îÄ‚îÄ R√©compense d'action: Encouragement d'actions l√©gales
```

### Architecture du R√©seau Neuronal
- **Couche d'entr√©e** : Vecteur d'√©tat de 820 dimensions
- **Couches cach√©es** : Multiples couches enti√®rement connect√©es, activation ReLU
- **Couche de sortie** : Pr√©diction de valeurs Q de 20 000 dimensions
- **Optimiseur** : Adam, taux d'apprentissage dynamique
- **R√©gularisation** : Dropout, √©cr√™tage de gradient

## üìà M√©triques de Performance

### Efficacit√© d'Entra√Ænement
- **Vitesse parall√®le** : >1000 √©pisodes/heure
- **Temps de convergence** : 3-4 heures pour compl√©ter l'apprentissage par curriculum
- **Efficacit√© d'√©chantillons** : Niveau expert atteint en 120k √©pisodes

### Capacit√©s de l'IA
- **Performance de taux de victoire** : >90% de taux de victoire contre joueurs al√©atoires
- **Profondeur strat√©gique** : Profondeur de r√©flexion moyenne de 15-20 coups
- **Vitesse de r√©action** : <0.1 seconde/coup

### Stabilit√©
- **Variance des r√©compenses** : <0.1 en fin d'entra√Ænement
- **Coh√©rence strat√©gique** : >95% de taux de reproduction des d√©cisions pour la m√™me situation
- **Robustesse** : Maintien de haute performance sous perturbations adversariales

## üîß Configuration Avanc√©e

### R√©compenses Personnalis√©es
```python
# Cr√©er un fa√ßonneur de r√©compenses personnalis√©
from improved_reward_shaping import HiveRewardShaper

shaper = HiveRewardShaper('custom')
shaper.config['terminal_weight'] = 0.7  # Augmenter le poids des r√©compenses terminales
shaper.config['strategy_weight'] = 0.3  # Ajuster le poids des r√©compenses strat√©giques
```

### Optimisation des Param√®tres d'Entra√Ænement
```python
# Ajuster les hyperparam√®tres dans ai_trainer.py
batch_size = 32          # Taille de lot
learning_rate = 0.001    # Taux d'apprentissage
epsilon_start = 0.9      # Taux d'exploration initial
epsilon_end = 0.05       # Taux d'exploration final
discount_factor = 0.95   # Facteur de remise
```

### Configuration Parall√®le
```python
# Ajuster le nombre de workers parall√®les
num_workers = 10         # Ajuster selon le nombre de c≈ìurs CPU
episodes_per_worker = 100 # Nombre d'√©pisodes par worker
queue_maxsize = 100      # Taille de la file d'attente
```

## üêõ D√©pannage

### Probl√®mes Courants
1. **Entra√Ænement lent**
   - V√©rifier la configuration des workers parall√®les
   - Confirmer que la file d'attente n'est pas bloqu√©e
   - V√©rifier la transmission correcte du reward_shaper

2. **Comportement anormal de l'IA**
   - V√©rifier la configuration du syst√®me de r√©compenses
   - Valider la raisonnabilit√© des statistiques terminales
   - Analyser la courbe de d√©croissance d'epsilon

3. **M√©moire insuffisante**
   - R√©duire batch_size
   - Ajuster la taille du tampon de replay d'exp√©rience
   - Utiliser moins de workers parall√®les

### Outils de D√©bogage
```bash
# Analyser le dernier mod√®le d'entra√Ænement
python analyze_model.py

# Visualiser les courbes d'entra√Ænement
python plot_reward_curve.py

# Tester la configuration de l'environnement
python test_environment.py
```

## ü§ù Guide de Contribution

Nous accueillons les contributions de la communaut√© ! Veuillez consulter les directives suivantes :

### Environnement de D√©veloppement
```bash
# Cloner le d√©p√¥t
git clone <repository-url>
cd Hive-RL

# Cr√©er un environnement virtuel
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou venv\Scripts\activate  # Windows

# Installer les d√©pendances de d√©veloppement
pip install -r requirements.txt
pip install -r requirements-dev.txt
```

### Standards de Code
- Suivre le style de code PEP 8
- Ajouter des annotations de type
- √âcrire des tests unitaires
- Mettre √† jour la documentation

### Processus de Soumission
1. Fork le projet
2. Cr√©er une branche de fonctionnalit√©
3. Committer les changements de code
4. Cr√©er une Pull Request

## üìÑ Licence

Ce projet est sous licence MIT. Voir le fichier [LICENSE](LICENSE) pour plus de d√©tails.

## üôè Remerciements

- **Jeu Hive** con√ßu par John Yianni
- Merci aux communaut√©s open source PyTorch et Gymnasium
- Remerciements sp√©ciaux √† tous les contributeurs et utilisateurs testeurs

## üìû Contact

- **Issues** : [GitHub Issues](../../issues)
- **Discussions** : [GitHub Discussions](../../discussions)
- **Email** : [your-email@example.com](mailto:your-email@example.com)

---

**Hive-RL** : O√π l'IA rencontre l'√©l√©gance du Hive ! üêù‚ôüÔ∏èü§ñ
