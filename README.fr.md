[üá®üá≥ ‰∏≠Êñá](README.md) | [üá¨üáß English](README.en.md) | [üá™üá∏ Espa√±ol](README.es.md)

# Hive-RL : Une IA pour le jeu de soci√©t√© Hive bas√©e sur l'Apprentissage par Renforcement

## Introduction

Hive-RL est un projet Python bas√© sur l'Apprentissage par Renforcement (AR) qui vise √† entra√Æner une Intelligence Artificielle (IA) de haut niveau pour le jeu de soci√©t√© **Hive**. Ce projet met en ≈ìuvre la logique compl√®te du jeu, un environnement d'apprentissage par renforcement compatible avec la norme OpenAI Gym/Gymnasium, et un entra√Æneur d'IA utilisant un R√©seau Q Profond (DQN).

## Caract√©ristiques du Projet

* **Impl√©mentation Compl√®te du Jeu** : Impl√©mente avec pr√©cision les r√®gles de base de Hive et le mouvement de toutes les pi√®ces, y compris les **pi√®ces d'extension DLC** officielles (Coccinelle, Moustique, Cloporte).
* **Architecture Modulaire** : Le code est clairement structur√© en modules pour la logique du jeu, l'environnement AR, le joueur IA, l'entra√Æneur, l'√©valuateur, etc., ce qui le rend facile √† comprendre et √† √©tendre.
* **Pilot√© par l'Apprentissage par Renforcement** : Utilise un R√©seau Q Profond (DQN) comme algorithme de base, permettant √† l'IA d'apprendre √† partir de z√©ro et d'√©voluer continuellement gr√¢ce √† l'Auto-Jeu (Self-Play) et √† diverses strat√©gies d'entra√Ænement avanc√©es.
* **Strat√©gies d'Entra√Ænement Avanc√©es** :
  * **Auto-Jeu Parall√®le** : Utilise le multiprocessing pour √©chantillonner en parall√®le, acc√©l√©rant consid√©rablement l'entra√Ænement.
  * **Apprentissage Curriculaire** : Permet √† l'IA de commencer √† apprendre √† partir de t√¢ches simplifi√©es (par exemple, apprendre √† placer la Reine des Abeilles en premier) et de passer progressivement au jeu complet, am√©liorant l'efficacit√© de l'apprentissage.
  * **Entra√Ænement Antagoniste** : Am√©liore la robustesse de l'IA en jouant contre un adversaire qui choisit sp√©cifiquement les "pires" coups.
  * **Entra√Ænement d'Ensemble** : Entra√Æne plusieurs mod√®les d'IA ind√©pendants et utilise le vote lors de la prise de d√©cision pour am√©liorer la pr√©cision et la stabilit√© des choix.
* **Visualisation et √âvaluation** : Fournit divers outils de visualisation pour tracer les courbes de r√©compense, les courbes de perte, les courbes de taux de victoire et d'autres statistiques pendant le processus d'entra√Ænement, ce qui facilite l'analyse des progr√®s d'apprentissage de l'IA.
* **Interface Conviviale** : Offre un menu principal en ligne de commande qui prend en charge divers modes, y compris Humain contre Humain, Humain contre IA, Entra√Ænement de l'IA et √âvaluation de l'IA.

## Architecture du Projet

* `main.py` : Le point d'entr√©e principal du projet, fournissant un menu interactif en ligne de commande.
* `game.py` : La logique principale du jeu, g√©rant le d√©roulement du jeu, les joueurs et les tours.
* `board.py` : Repr√©sentation du plateau et op√©rations de base.
* `piece.py` : D√©finit les propri√©t√©s et les r√®gles de mouvement pour toutes les pi√®ces (y compris le DLC).
* `player.py` : La classe de base pour les joueurs, g√©rant la main et les actions de base.
* `ai_player.py` : La classe du joueur IA, impl√©mentant la s√©lection d'actions et la relecture d'exp√©rience bas√©es sur un r√©seau de neurones.
* `hive_env.py` : L'environnement de jeu Hive suivant l'API Gymnasium, utilis√© pour l'entra√Ænement par apprentissage par renforcement.
* `neural_network.py` : Une impl√©mentation de r√©seau de neurones profond bas√©e sur PyTorch.
* `ai_trainer.py` : L'entra√Æneur d'IA, comprenant divers modes d'entra√Ænement (auto-jeu parall√®le, apprentissage curriculaire, entra√Ænement antagoniste, etc.).
* `ai_evaluator.py` : L'√©valuateur d'IA, utilis√© pour tester le taux de victoire de l'IA contre un joueur al√©atoire.
* `utils.py` : Fournit des fonctions et des outils d'aide.
* `requirements.txt` : Biblioth√®ques de d√©pendances du projet.

## Comment Ex√©cuter

### 1. Configuration de l'Environnement

Tout d'abord, assurez-vous d'avoir install√© Python 3.10 ou une version ult√©rieure. Ensuite, installez toutes les d√©pendances requises :

```bash
pip install -r requirements.txt
```

### 2. Ex√©cuter le Programme Principal

D√©marrez le menu principal du projet avec la commande suivante :

```bash
python main.py
```

Vous verrez les options suivantes :

1. **Humain contre Humain** : Jouez contre un autre joueur local.
2. **Humain contre IA** : Jouez contre une IA entra√Æn√©e.
3. **Entra√Ænement de l'IA** : Entra√Ænez un nouveau mod√®le d'IA ou continuez l'entra√Ænement √† partir d'un point de contr√¥le.
4. **√âvaluer l'IA & Graphiques** : √âvaluez les performances de l'IA et tracez les courbes d'entra√Ænement.
5. **Quitter le Jeu** : Quittez le programme.

### 3. Entra√Æner l'IA

* S√©lectionnez l'option `Entra√Ænement de l'IA` dans le menu principal.
* Vous pouvez choisir de **commencer un nouvel entra√Ænement** ou de **continuer √† partir d'un point de contr√¥le pr√©c√©dent**.
* Ensuite, s√©lectionnez un mode d'entra√Ænement, tel que **l'entra√Ænement de base avec √©chantillonnage parall√®le** ou **l'auto-jeu**.
* Pendant l'entra√Ænement, le mod√®le et les statistiques seront automatiquement sauvegard√©s dans le r√©pertoire `models/`, dans un dossier nomm√© avec un horodatage et le statut du DLC.
* Vous pouvez interrompre l'entra√Ænement √† tout moment avec `Ctrl+C`, et le programme sauvegardera automatiquement la progression actuelle pour la reprendre plus tard.

### 4. Jouer Contre l'IA

* S√©lectionnez l'option `Humain contre IA` dans le menu principal.
* Le programme listera automatiquement tous les mod√®les d'IA disponibles dans le r√©pertoire `models/`. Vous pouvez en choisir un pour jouer contre.
* Pendant le jeu, entrez vos coups comme demand√©.

## Principes de l'Apprentissage par Renforcement

L'IA de ce projet est bas√©e sur un **R√©seau Q Profond (DQN)**, un algorithme d'apprentissage par renforcement bas√© sur la valeur. L'id√©e principale est d'entra√Æner un r√©seau de neurones √† approximer la **fonction Q** `Q(s, a)`, qui pr√©dit le retour √† long terme (r√©compense) de l'action `a` dans un √©tat donn√© `s`.

* **√âtat** : Une repr√©sentation vectorielle de la situation de jeu actuelle, comprenant le type de pi√®ce √† chaque position sur le plateau, le nombre de pi√®ces restantes dans la main de chaque joueur, le num√©ro du tour actuel, etc.
* **Action** : L'une de toutes les op√©rations l√©gales de "placement" ou de "d√©placement".
* **R√©compense** : Le signal de retour que l'IA re√ßoit de l'environnement apr√®s avoir effectu√© une action.
  * **Gagner** : Re√ßoit une grande r√©compense positive.
  * **Perdre** : Re√ßoit une grande r√©compense n√©gative.
  * **Match Nul** : Re√ßoit une r√©compense nulle ou une petite r√©compense positive/n√©gative.
  * **Mise en Forme de la R√©compense (Reward Shaping)** : Pour guider l'IA √† apprendre plus rapidement, nous avons con√ßu une s√©rie de r√©compenses interm√©diaires, telles que :
    * Une r√©compense positive pour avoir encercl√© la Reine des Abeilles de l'adversaire.
    * Une p√©nalit√© si sa propre Reine des Abeilles est encercl√©e.
    * Une petite r√©compense positive pour avoir effectu√© un mouvement ou un placement l√©gal.
    * Une tr√®s petite r√©compense n√©gative pour chaque pas effectu√©, afin d'encourager l'IA √† gagner le plus rapidement possible.
* **Processus d'Entra√Ænement** :
  1. **√âchantillonnage** : L'IA (ou plusieurs IA parall√®les) joue le jeu par auto-jeu dans l'environnement, collectant un grand nombre de tuples d'exp√©rience `(√©tat, action, r√©compense, √©tat_suivant)`.
  2. **Relecture d'Exp√©rience** : Les exp√©riences collect√©es sont stock√©es dans un "pool d'exp√©riences".
  3. **Entra√Ænement** : Un petit lot d'exp√©riences est tir√© au hasard du pool d'exp√©riences pour entra√Æner le r√©seau de neurones. L'objectif de l'entra√Ænement est de rendre la valeur pr√©dite de `Q(s, a)` aussi proche que possible de la **valeur Q cible** (g√©n√©ralement `r√©compense + facteur_de_r√©duction * max(Q(√©tat_suivant, toutes_les_actions_l√©gales))`).
  4. **Exploration vs. Exploitation** : L'IA utilise une strat√©gie **Œµ-greedy** pour s√©lectionner les actions. C'est-√†-dire qu'avec une probabilit√© de Œµ, elle choisit une action l√©gale al√©atoire (exploration), et avec une probabilit√© de 1-Œµ, elle choisit l'action avec la valeur Q la plus √©lev√©e (exploitation). Au fur et √† mesure que l'entra√Ænement progresse, Œµ diminue progressivement, ce qui am√®ne l'IA √† passer de l'exploration al√©atoire √† une d√©pendance accrue aux strat√©gies optimales qu'elle a apprises.

Gr√¢ce √† des dizaines de milliers de parties d'auto-jeu et d'entra√Ænement, le r√©seau de neurones de l'IA peut progressivement apprendre les motifs et les strat√©gies complexes du plateau de Hive, atteignant ainsi un haut niveau de jeu.
