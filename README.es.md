[游뻟릖 疸쇉둖](README.md) | [游섫릖 English](README.en.md) | [游游 Fran칞ais](README.fr.md)

# Hive-RL: Una IA para el juego de mesa Hive basada en Aprendizaje por Refuerzo

## Introducci칩n

Hive-RL es un proyecto de Python basado en Aprendizaje por Refuerzo (RL) que tiene como objetivo entrenar una Inteligencia Artificial (IA) de alto nivel para el juego de mesa **Hive**. Este proyecto implementa la l칩gica completa del juego, un entorno de aprendizaje por refuerzo compatible con el est치ndar OpenAI Gym/Gymnasium, y un entrenador de IA que utiliza una Red Q Profunda (DQN).

## Caracter칤sticas del Proyecto

* **Implementaci칩n Completa del Juego**: Implementa con precisi칩n las reglas b치sicas de Hive y el movimiento de todas las piezas, incluidas las **piezas de expansi칩n DLC** oficiales (Mariquita, Mosquito, Cochinilla).
* **Arquitectura Modular**: El c칩digo est치 claramente estructurado en m칩dulos para la l칩gica del juego, el entorno de RL, el jugador de IA, el entrenador, el evaluador, etc., lo que facilita su comprensi칩n y ampliaci칩n.
* **Impulsado por Aprendizaje por Refuerfo**: Utiliza una Red Q Profunda (DQN) como su algoritmo principal, lo que permite a la IA aprender desde cero y evolucionar continuamente a trav칠s del Auto-Juego (Self-Play) y diversas estrategias de entrenamiento avanzadas.
* **Estrategias de Entrenamiento Avanzadas**:
  * **Auto-Juego Paralelo**: Utiliza el multiprocesamiento para muestrear en paralelo, acelerando significativamente el entrenamiento.
  * **Aprendizaje Curricular**: Permite a la IA comenzar a aprender de tareas simplificadas (por ejemplo, aprender a colocar la Abeja Reina primero) y pasar gradualmente al juego completo, mejorando la eficiencia del aprendizaje.
  * **Entrenamiento Adversario**: Mejora la robustez de la IA jugando contra un oponente que elige espec칤ficamente los "peores" movimientos.
  * **Entrenamiento de Conjunto**: Entrena m칰ltiples modelos de IA independientes y utiliza la votaci칩n durante la toma de decisiones para mejorar la precisi칩n y la estabilidad de las elecciones.
* **Visualizaci칩n y Evaluaci칩n**: Proporciona diversas herramientas de visualizaci칩n para trazar curvas de recompensa, curvas de p칠rdida, curvas de tasa de victorias y otras estad칤sticas durante el proceso de entrenamiento, lo que facilita el an치lisis del progreso de aprendizaje de la IA.
* **Interfaz F치cil de Usar**: Ofrece un men칰 principal de l칤nea de comandos que admite varios modos, incluidos Humano vs. Humano, Humano vs. IA, Entrenamiento de IA y Evaluaci칩n de IA.

## Arquitectura del Proyecto

* `main.py`: El punto de entrada principal del proyecto, que proporciona un men칰 interactivo de l칤nea de comandos.
* `game.py`: La l칩gica principal del juego, que gestiona el flujo del juego, los jugadores y los turnos.
* `board.py`: Representaci칩n del tablero y operaciones b치sicas.
* `piece.py`: Define las propiedades y las reglas de movimiento de todas las piezas (incluido el DLC).
* `player.py`: La clase base para los jugadores, que gestiona la mano y las acciones b치sicas.
* `ai_player.py`: La clase del jugador de IA, que implementa la selecci칩n de acciones y la repetici칩n de experiencias basadas en una red neuronal.
* `hive_env.py`: El entorno del juego Hive que sigue la API de Gymnasium, utilizado para el entrenamiento por aprendizaje por refuerzo.
* `neural_network.py`: Una impl칠mentaci칩n de red neuronal profunda basada en PyTorch.
* `ai_trainer.py`: El entrenador de IA, que incluye varios modos de entrenamiento (auto-juego paralelo, aprendizaje curricular, entrenamiento adversario, etc.).
* `ai_evaluator.py`: El evaluador de IA, utilizado para probar la tasa de victorias de la IA contra un jugador aleatorio.
* `utils.py`: Proporciona funciones y herramientas de ayuda.
* `requirements.txt`: Bibliotecas de dependencias del proyecto.

## C칩mo Ejecutar

### 1. Configuraci칩n del Entorno

Primero, aseg칰rese de tener instalado Python 3.10 o superior. Luego, instale todas las dependencias requeridas:

```bash
pip install -r requirements.txt
```

### 2. Ejecutar el Programa Principal

Inicie el men칰 principal del proyecto con el siguiente comando:

```bash
python main.py
```

Ver치 las siguientes opciones:

1. **Humano vs Humano**: Juega contra otro jugador local.
2. **Humano vs IA**: Juega contra una IA entrenada.
3. **Entrenamiento de IA**: Entrena un nuevo modelo de IA o contin칰a el entrenamiento desde un punto de control.
4. **Evaluar IA y Gr치ficos**: Eval칰a el rendimiento de la IA y traza las curvas de entrenamiento.
5. **Salir del Juego**: Salir del programa.

### 3. Entrenar la IA

* Seleccione la opci칩n `Entrenamiento de IA` en el men칰 principal.
* Puede elegir **iniciar un nuevo entrenamiento** o **continuar desde un punto de control anterior**.
* A continuaci칩n, seleccione un modo de entrenamiento, como **entrenamiento b치sico con muestreo paralelo** o **auto-juego**.
* Durante el entrenamiento, el modelo y las estad칤sticas se guardar치n autom치ticamente en el directorio `models/`, en una carpeta nombrada con una marca de tiempo y el estado del DLC.
* Puede interrumpir el entrenamiento en cualquier momento con `Ctrl+C`, y el programa guardar치 autom치ticamente el progreso actual para reanudarlo m치s tarde.

### 4. Jugar Contra la IA

* Seleccione la opci칩n `Humano vs IA` en el men칰 principal.
* El programa listar치 autom치ticamente todos los modelos de IA disponibles en el directorio `models/`. Puede elegir uno para jugar en su contra.
* Durante el juego, ingrese sus movimientos como se le solicite.

## Principios de Aprendizaje por Refuerzo

La IA de este proyecto se basa en una **Red Q Profunda (DQN)**, un algoritmo de aprendizaje por refuerzo impulsado por el valor. La idea central es entrenar una red neuronal para aproximar la **funci칩n Q** `Q(s, a)`, que predice el rendimiento a largo plazo (recompensa) de realizar la acci칩n `a` en un estado dado `s`.

* **Estado**: Una representaci칩n vectorizada de la situaci칩n actual del juego, que incluye el tipo de pieza en cada posici칩n del tablero, el n칰mero de piezas restantes en la mano de cada jugador, el n칰mero de turno actual, etc.
* **Acci칩n**: Una de todas las operaciones legales de "colocar" o "mover".
* **Recompensa**: La se침al de retroalimentaci칩n que la IA recibe del entorno despu칠s de realizar una acci칩n.
  * **Ganar**: Recibe una gran recompensa positiva.
  * **Perder**: Recibe una gran recompensa negativa.
  * **Empate**: Recibe una recompensa de cero o una peque침a recompensa positiva/negativa.
  * **Modelado de Recompensas (Reward Shaping)**: Para guiar a la IA a aprender m치s r치pido, dise침amos una serie de recompensas intermedias, como:
    * Una recompensa positiva por rodear a la Abeja Reina del oponente.
    * Una penalizaci칩n por tener la propia Abeja Reina rodeada.
    * Una peque침a recompensa positiva por realizar un movimiento o colocaci칩n legal.
    * Una peque침a recompensa negativa por cada paso dado, para alentar a la IA a ganar lo m치s r치pido posible.
* **Proceso de Entrenamiento**:
  1. **Muestreo**: La IA (o m칰ltiples IA paralelas) juega el juego a trav칠s del auto-juego en el entorno, recolectando una gran cantidad de tuplas de experiencia `(estado, acci칩n, recompensa, siguiente_estado)`.
  2. **Repetici칩n de Experiencias**: Las experiencias recolectadas se almacenan en un "grupo de experiencias".
  3. **Entrenamiento**: Se extrae un peque침o lote de experiencias al azar del grupo de experiencias para entrenar la red neuronal. El objetivo del entrenamiento es hacer que el valor predicho de `Q(s, a)` sea lo m치s cercano posible al **valor Q objetivo** (generalmente `recompensa + factor_de_descuento * max(Q(siguiente_estado, todas_las_acciones_legales))`).
  4. **Exploraci칩n vs. Explotaci칩n**: La IA utiliza una estrategia **풧-greedy** para seleccionar acciones. Es decir, con una probabilidad de 풧, elige una acci칩n legal aleatoria (exploraci칩n), y con una probabilidad de 1-풧, elige la acci칩n con el valor Q m치s alto (explotaci칩n). A medida que avanza el entrenamiento, 풧 decae gradualmente, lo que hace que la IA pase de la exploraci칩n aleatoria a depender m치s de las estrategias 칩ptimas que ha aprendido.

A trav칠s de decenas de miles de partidas de auto-juego y entrenamiento, la red neuronal de la IA puede aprender gradualmente los complejos patrones y estrategias del tablero de Hive, alcanzando as칤 un alto nivel de juego.

