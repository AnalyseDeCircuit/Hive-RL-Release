[üá¨üáß English](README.en.md) | [üá®üá≥ ‰∏≠Êñá](README.md) | [üá´üá∑ Fran√ßais](README.fr.md)

# Hive-RL: IA de Hive basada en Aprendizaje por Refuerzo

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

## üìñ Introducci√≥n

Hive-RL es un proyecto avanzado de aprendizaje por refuerzo dedicado al entrenamiento de una IA de alto nivel para el juego **Hive**. Este proyecto utiliza t√©cnicas modernas de aprendizaje por refuerzo profundo, implementando un motor de juego completo, un sistema de recompensas cient√≠fico y diversos algoritmos de entrenamiento avanzados.

**Hive** es un juego de estrategia galardonado que no requiere tablero, con reglas simples pero una profundidad estrat√©gica excepcional. El objetivo de los jugadores es rodear a la reina enemiga colocando y moviendo varias piezas de insectos.

## ‚ú® Caracter√≠sticas Principales

### üéÆ Motor de Juego Completo

- **Implementaci√≥n precisa de reglas**: Conforme a las reglas oficiales de Hive
- **Soporte de extensiones DLC**: Incluye mariquita, mosquito, cochinilla y otras piezas oficiales
- **Tablero de alto rendimiento**: Estructuras de datos optimizadas y aceleraci√≥n Numba
- **Validaci√≥n de acciones**: Verificaci√≥n estricta de legalidad y manejo de errores

### üß† Sistema de IA Avanzado

- **Deep Q-Network (DQN)**: Arquitectura de red neuronal moderna basada en PyTorch
- **Modelado de recompensas cient√≠fico**: Sistema de recompensas multinivel cuidadosamente dise√±ado
- **Replay de experiencia**: Reutilizaci√≥n eficiente de muestras y estabilidad de aprendizaje
- **Estrategia Œµ-greedy**: Estrategia din√°mica que equilibra exploraci√≥n y explotaci√≥n

### üöÄ Framework de Entrenamiento de Alto Rendimiento

- **Auto-juego paralelo**: Muestreo paralelo multiproceso, mejora significativa de la eficiencia de entrenamiento
- **Aprendizaje curricular**: Aprendizaje progresivo desde reglas b√°sicas hasta estrategias avanzadas
- **Entrenamiento adversarial**: Mejora de la robustez de la IA mediante muestras adversariales
- **Fusi√≥n de modelos**: Sistema de decisi√≥n por votaci√≥n multimodelo

### üìä Visualizaci√≥n y An√°lisis

- **Monitoreo en tiempo real**: Curvas de recompensas, p√©rdidas y tasas de victoria durante el entrenamiento
- **An√°lisis de rendimiento**: Estad√≠sticas detalladas de final de juego y an√°lisis de comportamiento
- **Evaluaci√≥n de modelo**: Pruebas de rendimiento automatizadas

## üèóÔ∏è Arquitectura del Proyecto

```
Hive-RL/
‚îú‚îÄ‚îÄ Motor Principal
‚îÇ   ‚îú‚îÄ‚îÄ game.py              # L√≥gica principal del juego
‚îÇ   ‚îú‚îÄ‚îÄ board.py             # Representaci√≥n y operaciones del tablero
‚îÇ   ‚îú‚îÄ‚îÄ piece.py             # Tipos de piezas y reglas de movimiento
‚îÇ   ‚îî‚îÄ‚îÄ player.py            # Clase base de jugadores
‚îú‚îÄ‚îÄ Aprendizaje por Refuerzo
‚îÇ   ‚îú‚îÄ‚îÄ hive_env.py          # Entorno Gymnasium
‚îÇ   ‚îú‚îÄ‚îÄ ai_player.py         # Implementaci√≥n del jugador IA
‚îÇ   ‚îú‚îÄ‚îÄ neural_network.py    # Arquitectura de red neuronal
‚îÇ   ‚îî‚îÄ‚îÄ improved_reward_shaping.py  # Sistema de modelado de recompensas
‚îú‚îÄ‚îÄ Framework de Entrenamiento
‚îÇ   ‚îú‚îÄ‚îÄ ai_trainer.py        # Entrenador principal
‚îÇ   ‚îú‚îÄ‚îÄ parallel_sampler.py  # Muestreador paralelo
‚îÇ   ‚îî‚îÄ‚îÄ ai_evaluator.py      # Evaluador de rendimiento
‚îú‚îÄ‚îÄ Herramientas de An√°lisis
‚îÇ   ‚îú‚îÄ‚îÄ analyze_model.py     # An√°lisis de modelo
‚îÇ   ‚îî‚îÄ‚îÄ plot_*.py           # Herramientas de visualizaci√≥n
‚îî‚îÄ‚îÄ Interfaz de Usuario
    ‚îî‚îÄ‚îÄ main.py             # Men√∫ principal
```

## üöÄ Inicio R√°pido

### Requisitos Previos

- Python 3.10+
- PyTorch 2.0+
- NumPy, Matplotlib, Gymnasium
- Numba (optimizaci√≥n de rendimiento)

### Instalaci√≥n de Dependencias

```bash
pip install -r requirements.txt
```

### Lanzamiento del Proyecto

```bash
python main.py
```

### Opciones del Men√∫ Principal

1. **Human vs Human** - Combate local de dos jugadores
2. **Human vs AI** - Combate humano-m√°quina
3. **AI Training** - Entrenamiento de IA
4. **Evaluate AI & Plots** - Evaluaci√≥n de rendimiento
5. **Exit Game** - Salir

## üéØ Entrenamiento de IA

### Modos de Entrenamiento

1. **Entrenamiento b√°sico por muestreo paralelo** - Entrenamiento multiproceso eficiente
2. **Entrenamiento de refinamiento por auto-juego** - Optimizaci√≥n estrat√©gica profunda
3. **Entrenamiento por votaci√≥n en conjunto** - Fusi√≥n multimodelo
4. **Entrenamiento de robustez adversarial** - Mejora de resistencia a perturbaciones
5. **Aprendizaje curricular** - Adquisici√≥n progresiva de habilidades

### Fases del Aprendizaje Curricular

- **Foundation (0-40k episodios)** - Aprendizaje de reglas b√°sicas
- **Strategy (40k-90k episodios)** - Desarrollo del pensamiento estrat√©gico
- **Mastery (90k-120k episodios)** - Dominio de estrategias avanzadas

### Caracter√≠sticas del Entrenamiento

- **Guardado autom√°tico**: Progreso de entrenamiento guardado en tiempo real, soporte de reanudaci√≥n
- **Monitoreo de rendimiento**: Visualizaci√≥n en tiempo real de velocidad de entrenamiento y estado de convergencia
- **Programaci√≥n inteligente**: Ajuste din√°mico de epsilon y tasa de aprendizaje
- **Optimizaci√≥n multiproceso**: 10 workers paralelos, mejora de velocidad de entrenamiento 10x

## üî¨ Principios T√©cnicos

### Framework de Aprendizaje por Refuerzo

- **Espacio de estados**: Vector de 820 dimensiones incluyendo estado del tablero, informaci√≥n de mano, progreso del juego
- **Espacio de acciones**: 20,000 acciones discretas cubriendo todas las colocaciones y movimientos posibles
- **Sistema de recompensas**: Dise√±o de recompensas multinivel, desde supervivencia b√°sica hasta estrategias avanzadas

### Sistema de Modelado de Recompensas

```python
Recompensas Terminales (Peso: 60-63%)
‚îú‚îÄ‚îÄ Victoria: +5.0 + bonus de velocidad
‚îú‚îÄ‚îÄ Derrota: -6.0 (reina rodeada)
‚îú‚îÄ‚îÄ Timeout: -3.0 (penalizaci√≥n por demora)
‚îî‚îÄ‚îÄ Empate: Ajuste fino seg√∫n ventaja

Recompensas Estrat√©gicas (Peso: 25-40%)
‚îú‚îÄ‚îÄ Progreso de cerco: Recompensas progresivas
‚îú‚îÄ‚îÄ Mejora defensiva: Recompensas de posici√≥n segura
‚îî‚îÄ‚îÄ Coordinaci√≥n de piezas: Evaluaci√≥n de valor posicional

Recompensas B√°sicas (Peso: 5-15%)
‚îú‚îÄ‚îÄ Recompensa de supervivencia: Valor positivo m√≠nimo
‚îî‚îÄ‚îÄ Recompensa de acci√≥n: Est√≠mulo de acciones legales
```

### Arquitectura de Red Neuronal

- **Capa de entrada**: Vector de estado de 820 dimensiones
- **Capas ocultas**: M√∫ltiples capas completamente conectadas, activaci√≥n ReLU
- **Capa de salida**: Predicci√≥n de valores Q de 20,000 dimensiones
- **Optimizador**: Adam, tasa de aprendizaje din√°mica
- **Regularizaci√≥n**: Dropout, recorte de gradientes

## üìà M√©tricas de Rendimiento

### Eficiencia de Entrenamiento

- **Velocidad paralela**: >1000 episodios/hora
- **Tiempo de convergencia**: 3-4 horas para completar aprendizaje curricular
- **Eficiencia de muestras**: Nivel experto alcanzado en 120k episodios

### Capacidades de IA

- **Rendimiento de tasa de victoria**: >90% de tasa de victoria contra jugadores aleatorios
- **Profundidad estrat√©gica**: Profundidad de pensamiento promedio de 15-20 movimientos
- **Velocidad de reacci√≥n**: <0.1 segundo/movimiento

### Estabilidad

- **Varianza de recompensas**: <0.1 al final del entrenamiento
- **Consistencia estrat√©gica**: >95% de tasa de reproducci√≥n de decisiones para la misma situaci√≥n
- **Robustez**: Mantenimiento de alto rendimiento bajo perturbaciones adversariales

## üîß Configuraci√≥n Avanzada

### Recompensas Personalizadas

```python
# Crear un modelador de recompensas personalizado
from improved_reward_shaping import HiveRewardShaper

shaper = HiveRewardShaper('custom')
shaper.config['terminal_weight'] = 0.7  # Aumentar peso de recompensas terminales
shaper.config['strategy_weight'] = 0.3  # Ajustar peso de recompensas estrat√©gicas
```

### Optimizaci√≥n de Par√°metros de Entrenamiento

```python
# Ajustar hiperpar√°metros en ai_trainer.py
batch_size = 32          # Tama√±o de lote
learning_rate = 0.001    # Tasa de aprendizaje
epsilon_start = 0.9      # Tasa de exploraci√≥n inicial
epsilon_end = 0.05       # Tasa de exploraci√≥n final
discount_factor = 0.95   # Factor de descuento
```

### Configuraci√≥n Paralela

```python
# Ajustar n√∫mero de workers paralelos
num_workers = 10         # Ajustar seg√∫n n√∫mero de n√∫cleos CPU
episodes_per_worker = 100 # N√∫mero de episodios por worker
queue_maxsize = 100      # Tama√±o de cola
```

## üêõ Soluci√≥n de Problemas

### Problemas Comunes

1. **Entrenamiento lento**
   - Verificar configuraci√≥n de workers paralelos
   - Confirmar que la cola no est√© bloqueada
   - Verificar transmisi√≥n correcta del reward_shaper

2. **Comportamiento anormal de IA**
   - Verificar configuraci√≥n del sistema de recompensas
   - Validar razonabilidad de estad√≠sticas terminales
   - Analizar curva de decaimiento epsilon

3. **Memoria insuficiente**
   - Reducir batch_size
   - Ajustar tama√±o del buffer de replay de experiencia
   - Usar menos workers paralelos

### Herramientas de Depuraci√≥n

```bash
# Analizar √∫ltimo modelo de entrenamiento
python analyze_model.py

# Visualizar curvas de entrenamiento
python plot_reward_curve.py

# Probar configuraci√≥n del entorno
python test_environment.py
```

## ü§ù Gu√≠a de Contribuci√≥n

¬°Damos la bienvenida a las contribuciones de la comunidad! Por favor consulte las siguientes pautas:

### Entorno de Desarrollo

```bash
# Clonar repositorio
git clone <repository-url>
cd Hive-RL

# Crear entorno virtual
python -m venv venv
source venv/bin/activate  # Linux/Mac
# o venv\Scripts\activate  # Windows

# Instalar dependencias de desarrollo
pip install -r requirements.txt
pip install -r requirements-dev.txt
```

### Est√°ndares de C√≥digo

- Seguir estilo de c√≥digo PEP 8
- Agregar anotaciones de tipo
- Escribir pruebas unitarias
- Actualizar documentaci√≥n

### Proceso de Env√≠o

1. Fork del proyecto
2. Crear rama de caracter√≠stica
3. Confirmar cambios de c√≥digo
4. Crear Pull Request

## üìÑ Licencia

Este proyecto est√° bajo licencia MIT. Ver archivo [LICENSE](LICENSE) para m√°s detalles.

## üôè Agradecimientos

- **Juego Hive** dise√±ado por John Yianni
- Gracias a las comunidades de c√≥digo abierto PyTorch y Gymnasium
- Agradecimientos especiales a todos los contribuidores y usuarios de prueba

## üìû Contacto

- **Issues**: [GitHub Issues](../../issues)
- **Discussions**: [GitHub Discussions](../../discussions)
- **Email**: [your-email@example.com](mailto:your-email@example.com)

---

**Hive-RL**: ¬°Donde la IA se encuentra con la elegancia del Hive! üêù‚ôüÔ∏èü§ñ

